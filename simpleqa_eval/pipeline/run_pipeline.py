"""SimpleQA è¯„æµ‹é“¾è·¯ä¸»æµæ°´çº¿

åˆ† 3 é˜¶æ®µå¯ç‹¬ç«‹è¿è¡Œï¼š
  1. generate  - å¯¹æ‰€æœ‰æ ·æœ¬åšå¤šæ¬¡é‡‡æ ·
  2. evaluate  - åˆ¤é”™ + å®ä½“æŠ½å– + Jaccard è®¡ç®—
  3. analyze   - åˆ†æ¡¶ç»Ÿè®¡ + ç”»å›¾

ç”¨æ³•ï¼š
  python -m simpleqa_eval.pipeline.run_pipeline --stage generate --first_n 200
  python -m simpleqa_eval.pipeline.run_pipeline --stage evaluate
  python -m simpleqa_eval.pipeline.run_pipeline --stage analyze
  python -m simpleqa_eval.pipeline.run_pipeline --stage all --first_n 200
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

import numpy as np

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€
def setup_logging(level: str = "INFO"):
    """é…ç½®æ—¥å¿—æ ¼å¼"""
    fmt = "%(asctime)s â”‚ %(levelname)-7s â”‚ %(message)s"
    datefmt = "%H:%M:%S"
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format=fmt,
        datefmt=datefmt,
        handlers=[logging.StreamHandler(sys.stdout)],
    )

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  é˜¶æ®µ 1: GENERATE â€” å¯¹ SimpleQA æ ·æœ¬åšå¤šæ¬¡é‡‡æ ·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stage_generate(cfg):
    """é˜¶æ®µ 1: æ¨¡å‹ç”Ÿæˆ"""
    from simpleqa_eval.data.simpleqa_loader import load_simpleqa
    from simpleqa_eval.models.generator import LocalModelGenerator

    raw_path = cfg.outputs_dir / "raw_generations.jsonl"

    print()
    logger.info("=" * 70)
    logger.info("  ğŸš€ é˜¶æ®µ 1: GENERATE â€” æ¨¡å‹å¤šæ¬¡é‡‡æ ·")
    logger.info("=" * 70)
    logger.info(f"  æ¨¡å‹:       {cfg.model_name}")
    logger.info(f"  é‡‡æ ·æ¬¡æ•°:    {cfg.n_samples}")
    logger.info(f"  æ¸©åº¦:       {cfg.temperature}")
    logger.info(f"  æœ€å¤§ token: {cfg.max_new_tokens}")
    logger.info(f"  æ•°æ®é‡:     å‰ {cfg.first_n} æ¡")
    logger.info(f"  è¾“å‡ºæ–‡ä»¶:   {raw_path}")
    logger.info("=" * 70)
    print()

    # åŠ è½½æ•°æ®
    data = load_simpleqa(first_n=cfg.first_n)
    logger.info(f"ğŸ“‹ åŠ è½½äº† {len(data)} æ¡ SimpleQA æ ·æœ¬")
    print()

    remaining = data
    logger.info(f"ğŸ“ å¾…å¤„ç†: {len(remaining)} æ¡")

    # åŠ è½½æ¨¡å‹
    generator = LocalModelGenerator(
        model_name=cfg.model_name,
        device=cfg.device,
        max_new_tokens=cfg.max_new_tokens,
    )
    generator.load()
    print()

    # é€æ¡ç”Ÿæˆ
    total = len(remaining)
    start_time = time.time()

    with open(raw_path, "w", encoding="utf-8") as f:
        for idx, sample in enumerate(remaining):
            qid = sample["qid"]
            question = sample["question"]
            gt = sample["ground_truth"]

            logger.info(f"â”€â”€â”€ [{idx+1}/{total}] qid={qid} â”€â”€â”€")
            logger.info(f"  â“ é—®é¢˜: {question[:80]}{'...' if len(question) > 80 else ''}")
            logger.info(f"  âœ… GT:   {gt[:60]}{'...' if len(gt) > 60 else ''}")

            # å¤šæ¬¡é‡‡æ ·
            logger.info(f"  ğŸ² å¼€å§‹ {cfg.n_samples} æ¬¡é‡‡æ · (temperature={cfg.temperature})...")
            sample_start = time.time()
            results = generator.sample_answers(
                question, n=cfg.n_samples, temperature=cfg.temperature
            )
            sample_elapsed = time.time() - sample_start

            # æå–ç»“æœ
            generations = [r[0] for r in results]
            confidences = [r[1] for r in results]

            # æ‰“å°æ¯æ¬¡é‡‡æ ·ç»“æœ
            for j, (ans, conf) in enumerate(results):
                logger.info(f"    [{j+1:2d}] conf={conf:.4f} | {ans[:60]}{'...' if len(ans) > 60 else ''}")

            logger.info(f"  â±ï¸  è€—æ—¶: {sample_elapsed:.1f}s")

            # ä¿å­˜
            record = {
                "qid": qid,
                "question": question,
                "ground_truth": gt,
                "generations": generations,
                "confidences": confidences,
                "model_name": cfg.model_name,
                "temperature": cfg.temperature,
                "n_samples": cfg.n_samples,
            }
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
            f.flush()

            # è¿›åº¦ä¼°ç®—
            elapsed = time.time() - start_time
            avg_per_sample = elapsed / (idx + 1)
            remaining_est = avg_per_sample * (total - idx - 1)
            logger.info(f"  ğŸ“Š è¿›åº¦: {idx+1}/{total} | å¹³å‡ {avg_per_sample:.1f}s/æ¡ | é¢„è®¡å‰©ä½™ {remaining_est/60:.1f}min")
            print()

    total_elapsed = time.time() - start_time
    logger.info(f"âœ… é˜¶æ®µ 1 å®Œæˆï¼å…±å¤„ç† {total} æ¡ï¼Œè€—æ—¶ {total_elapsed/60:.1f} åˆ†é’Ÿ")
    logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {raw_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  é˜¶æ®µ 2: EVALUATE â€” åˆ¤é”™ + å®ä½“æŠ½å– + Jaccard è®¡ç®—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stage_evaluate(cfg):
    """é˜¶æ®µ 2: è¯„ä¼°ä¸ Jaccard è®¡ç®—"""
    from simpleqa_eval.eval.answer_match import llm_judge_correct
    from simpleqa_eval.eval.consistency import compute_consensus_llm
    from simpleqa_eval.entity.extractor import extract_entities
    from simpleqa_eval.entity.wiki_proxy import WikiCache, get_article_set_for_entities
    from simpleqa_eval.entity.jaccard import jaccard_similarity

    raw_path = cfg.outputs_dir / "raw_generations.jsonl"
    result_csv = cfg.outputs_dir / "sample_results.csv"

    print()
    logger.info("=" * 70)
    logger.info("  ğŸ” é˜¶æ®µ 2: EVALUATE â€” åˆ¤é”™ + å®ä½“ + Jaccard")
    logger.info("=" * 70)
    logger.info(f"  è¾“å…¥:        {raw_path}")
    logger.info(f"  è¾“å‡º:        {result_csv}")
    logger.info(f"  DeepSeek:    {cfg.deepseek_model}")
    logger.info(f"  Wiki ç¼“å­˜:   {cfg.wiki_cache_db}")
    logger.info("=" * 70)
    print()

    # æ£€æŸ¥ API key
    if not cfg.deepseek_api_key:
        logger.error("âŒ æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼")
        logger.error("   è¯·è®¾ç½®: export DEEPSEEK_API_KEY='your-key-here'")
        sys.exit(1)

    # è¯»å–ç”Ÿæˆç»“æœ
    if not raw_path.exists():
        logger.error(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆç»“æœ: {raw_path}")
        logger.error("   è¯·å…ˆè¿è¡Œ --stage generate")
        sys.exit(1)

    records = []
    with open(raw_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                records.append(json.loads(line))
            except json.JSONDecodeError:
                pass

    # ä¸ generate é˜¶æ®µä¿æŒä¸€è‡´ï¼šæ”¯æŒ first_n å¿«é€Ÿå°æ ·æœ¬éªŒè¯
    if cfg.first_n > 0:
        original_n = len(records)
        records = records[:cfg.first_n]
        logger.info(f"âœ‚ï¸  first_n={cfg.first_n}ï¼šè¯„ä¼°å‰ {len(records)} æ¡ï¼ˆåŸå§‹ {original_n} æ¡ï¼‰")

    logger.info(f"ğŸ“‹ åŠ è½½äº† {len(records)} æ¡ç”Ÿæˆè®°å½•")
    print()

    remaining = records
    existing_rows = []
    logger.info(f"ğŸ“ å¾…å¤„ç†: {len(remaining)} æ¡")
    if not remaining:
        logger.warning("âš ï¸ æ— å¾…è¯„ä¼°æ ·æœ¬ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
        return
    print()

    # åˆå§‹åŒ– Wiki ç¼“å­˜
    wiki_cache = WikiCache(cfg.wiki_cache_db)

    # ç»Ÿè®¡è®¡æ•°å™¨
    stats = {
        "correct": 0,
        "incorrect": 0,
        "entity_fail": 0,
        "wiki_fail_samples": 0,
        "wiki_fail_entities": 0,
        "wiki_total_entities": 0,
    }
    total = len(remaining)
    start_time = time.time()

    for idx, record in enumerate(remaining):
        qid = record["qid"]
        question = record["question"]
        gt = record["ground_truth"]
        generations = record["generations"]
        confidences = record.get("confidences", [0.0] * len(generations))

        logger.info(f"â•â•â• [{idx+1}/{total}] qid={qid} â•â•â•")
        logger.info(f"  â“ é—®é¢˜: {question[:80]}{'...' if len(question) > 80 else ''}")
        logger.info(f"  âœ… GT:   {gt[:60]}{'...' if len(gt) > 60 else ''}")
        logger.info(f"  ğŸ“‹ æœ‰ {len(generations)} ä¸ªç”Ÿæˆç»“æœ")

        # â”€â”€ 2.1 è®¡ç®—å…±è¯†ç­”æ¡ˆå’Œè‡ªä¸€è‡´æ€§ â”€â”€
        logger.info("  ğŸ”— è®¡ç®—å…±è¯†ç­”æ¡ˆï¼ˆLLM èšç±»ï¼‰...")
        consensus_answer, consensus_count, self_consistency = compute_consensus_llm(
            generations,
            api_key=cfg.deepseek_api_key,
            base_url=cfg.deepseek_base_url,
            model=cfg.deepseek_model,
            retry_times=cfg.api_retry_times,
            retry_delay=cfg.api_retry_delay,
        )
        logger.info(f"  ğŸ“Š å…±è¯†ç­”æ¡ˆ: '{consensus_answer[:50]}{'...' if len(consensus_answer) > 50 else ''}'")
        logger.info(f"     å…±è¯†è®¡æ•°: {consensus_count}/{len(generations)} | self-consistency={self_consistency:.2f}")

        time.sleep(cfg.api_call_delay)

        # â”€â”€ 2.2 åˆ¤æ–­ consensus æ˜¯å¦æ­£ç¡® â”€â”€
        logger.info("  âš–ï¸  LLM åˆ¤é”™...")
        is_correct = llm_judge_correct(
            prediction=consensus_answer,
            ground_truth=gt,
            api_key=cfg.deepseek_api_key,
            base_url=cfg.deepseek_base_url,
            model=cfg.deepseek_model,
            retry_times=cfg.api_retry_times,
            retry_delay=cfg.api_retry_delay,
        )
        is_hallucination = 0 if is_correct else 1
        label = "âœ… æ­£ç¡®" if is_correct else "âŒ é”™è¯¯(å¹»è§‰)"
        logger.info(f"     åˆ¤å®š: {label}")
        if is_correct:
            stats["correct"] += 1
        else:
            stats["incorrect"] += 1

        time.sleep(cfg.api_call_delay)

        # â”€â”€ 2.3 å®ä½“æŠ½å– â”€â”€
        logger.info("  ğŸ·ï¸  æŠ½å–é—®é¢˜å®ä½“...")
        question_entities = extract_entities(
            question,
            api_key=cfg.deepseek_api_key,
            base_url=cfg.deepseek_base_url,
            model=cfg.deepseek_model,
            retry_times=cfg.api_retry_times,
            retry_delay=cfg.api_retry_delay,
        )
        logger.info(f"     é—®é¢˜å®ä½“: {question_entities}")

        time.sleep(cfg.api_call_delay)

        logger.info("  ğŸ·ï¸  æŠ½å–ç­”æ¡ˆå®ä½“...")
        consensus_entities = extract_entities(
            consensus_answer,
            api_key=cfg.deepseek_api_key,
            base_url=cfg.deepseek_base_url,
            model=cfg.deepseek_model,
            retry_times=cfg.api_retry_times,
            retry_delay=cfg.api_retry_delay,
        )
        logger.info(f"     ç­”æ¡ˆå®ä½“: {consensus_entities}")

        time.sleep(cfg.api_call_delay)

        # â”€â”€ 2.4 Wikipedia æ–‡ç« é›†åˆ + Jaccard â”€â”€
        jaccard_proxy = float("nan")
        q_article_size = 0
        a_article_size = 0
        wiki_failed = 0
        wiki_failed_entities = 0
        wiki_total_entities = 0

        if question_entities and consensus_entities:
            logger.info("  ğŸŒ æŸ¥è¯¢ Wikipedia æ–‡ç« é›†åˆ...")
            q_articles, q_failed, q_total = get_article_set_for_entities(
                question_entities, cache=wiki_cache,
                search_limit=cfg.wiki_search_limit,
            )
            a_articles, a_failed, a_total = get_article_set_for_entities(
                consensus_entities, cache=wiki_cache,
                search_limit=cfg.wiki_search_limit,
            )
            wiki_failed_entities = q_failed + a_failed
            wiki_total_entities = q_total + a_total
            stats["wiki_fail_entities"] += wiki_failed_entities
            stats["wiki_total_entities"] += wiki_total_entities
            q_article_size = len(q_articles)
            a_article_size = len(a_articles)
            if wiki_failed_entities > 0:
                wiki_failed = 1
                stats["wiki_fail_samples"] += 1
                logger.warning(
                    "     âš ï¸  Wikipedia è¯·æ±‚å¤±è´¥ï¼ŒJaccard è®¾ä¸º NaN "
                    f"(failed_entities={wiki_failed_entities}/{wiki_total_entities})"
                )
            else:
                jaccard_proxy = jaccard_similarity(q_articles, a_articles)

            logger.info(f"     é—®é¢˜æ–‡ç« é›†: {q_article_size} ç¯‡")
            logger.info(f"     ç­”æ¡ˆæ–‡ç« é›†: {a_article_size} ç¯‡")
            if np.isnan(jaccard_proxy):
                logger.info("     Jaccard = NaN")
            else:
                logger.info(f"     Jaccard = {jaccard_proxy:.4f}")
        else:
            stats["entity_fail"] += 1
            logger.warning(f"     âš ï¸  å®ä½“ä¸ºç©ºï¼ŒJaccard è®¾ä¸º NaNï¼ˆé—®é¢˜å®ä½“: {len(question_entities)}, ç­”æ¡ˆå®ä½“: {len(consensus_entities)}ï¼‰")

        # â”€â”€ 2.5 æ±‡æ€»å¹³å‡ confidence â”€â”€
        mean_confidence = float(np.mean(confidences)) if confidences else 0.0

        # â”€â”€ ä¿å­˜ä¸€æ¡ç»“æœ â”€â”€
        row = {
            "qid": qid,
            "question": question,
            "ground_truth": gt,
            "consensus_answer": consensus_answer,
            "consensus_count": consensus_count,
            "self_consistency": self_consistency,
            "confidence": mean_confidence,
            "is_correct": int(is_correct),
            "is_hallucination": is_hallucination,
            "question_entities": json.dumps(question_entities, ensure_ascii=False),
            "consensus_entities": json.dumps(consensus_entities, ensure_ascii=False),
            "question_article_set_size": q_article_size,
            "answer_article_set_size": a_article_size,
            "jaccard_proxy": jaccard_proxy,
            "wiki_failed": wiki_failed,
            "wiki_failed_entities": wiki_failed_entities,
            "wiki_total_entities": wiki_total_entities,
            "model_name": record.get("model_name", ""),
            "temperature": record.get("temperature", 0),
            "n_samples": record.get("n_samples", 0),
        }
        existing_rows.append(row)

        # æ¯æ¡éƒ½å†™ä¸€æ¬¡ CSVï¼ˆå¢é‡ä¿å­˜ï¼‰
        _write_csv(result_csv, existing_rows)

        # è¿›åº¦ä¸ç»Ÿè®¡
        elapsed = time.time() - start_time
        avg_per_sample = elapsed / (idx + 1)
        remaining_est = avg_per_sample * (total - idx - 1)
        total_processed = stats["correct"] + stats["incorrect"]
        halluc_rate = stats["incorrect"] / total_processed if total_processed > 0 else 0
        wiki_fail_rate = stats["wiki_fail_samples"] / total_processed if total_processed > 0 else 0

        logger.info(f"  ğŸ“Š ç´¯è®¡: æ­£ç¡®={stats['correct']} é”™è¯¯={stats['incorrect']} "
                     f"å¹»è§‰ç‡={halluc_rate:.2%} å®ä½“å¤±è´¥={stats['entity_fail']} "
                     f"Wikiå¤±è´¥ç‡={wiki_fail_rate:.2%}")
        logger.info(f"  â±ï¸  å¹³å‡ {avg_per_sample:.1f}s/æ¡ | é¢„è®¡å‰©ä½™ {remaining_est/60:.1f}min")
        print()

    wiki_cache.close()

    total_elapsed = time.time() - start_time
    total_processed = stats["correct"] + stats["incorrect"]
    halluc_rate = stats["incorrect"] / total_processed if total_processed > 0 else 0
    wiki_fail_rate = stats["wiki_fail_samples"] / total_processed if total_processed > 0 else 0
    wiki_entity_fail_rate = (
        stats["wiki_fail_entities"] / stats["wiki_total_entities"]
        if stats["wiki_total_entities"] > 0 else 0
    )

    print()
    logger.info("=" * 70)
    logger.info("  ğŸ“Š é˜¶æ®µ 2 æœ€ç»ˆç»Ÿè®¡")
    logger.info("=" * 70)
    logger.info(f"  æ€»æ ·æœ¬:     {total_processed}")
    logger.info(f"  æ­£ç¡®:       {stats['correct']}")
    logger.info(f"  é”™è¯¯(å¹»è§‰): {stats['incorrect']}")
    logger.info(f"  å¹»è§‰ç‡:     {halluc_rate:.2%}")
    logger.info(f"  å®ä½“æŠ½å–å¤±è´¥: {stats['entity_fail']}")
    logger.info(f"  Wiki æ ·æœ¬å¤±è´¥ç‡: {wiki_fail_rate:.2%} ({stats['wiki_fail_samples']}/{total_processed})")
    logger.info(
        "  Wiki å®ä½“å¤±è´¥ç‡: "
        f"{wiki_entity_fail_rate:.2%} ({stats['wiki_fail_entities']}/{stats['wiki_total_entities']})"
    )
    logger.info(f"  è€—æ—¶:       {total_elapsed/60:.1f} åˆ†é’Ÿ")
    logger.info(f"  ğŸ’¾ ç»“æœ:    {result_csv}")
    logger.info("=" * 70)


def _write_csv(path: Path, rows: List[Dict]):
    """å°†ç»“æœåˆ—è¡¨å†™å…¥ CSV"""
    if not rows:
        return
    fieldnames = rows[0].keys()
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  é˜¶æ®µ 3: ANALYZE â€” åˆ†æ¡¶ç»Ÿè®¡ + ç”»å›¾
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def stage_analyze(cfg):
    """é˜¶æ®µ 3: åˆ†æ¡¶åˆ†æä¸å¯è§†åŒ–"""
    from simpleqa_eval.pipeline.aggregate import aggregate_buckets
    from simpleqa_eval.viz.plot_trends import plot_bucket_trends

    result_csv = cfg.outputs_dir / "sample_results.csv"
    bucket_csv = cfg.outputs_dir / "bucket_stats.csv"
    figures_dir = cfg.outputs_dir / "figures"

    print()
    logger.info("=" * 70)
    logger.info("  ğŸ“Š é˜¶æ®µ 3: ANALYZE â€” åˆ†æ¡¶ç»Ÿè®¡ + å¯è§†åŒ–")
    logger.info("=" * 70)
    logger.info(f"  è¾“å…¥:   {result_csv}")
    logger.info(f"  æ¡¶ç»Ÿè®¡: {bucket_csv}")
    logger.info(f"  å›¾è¡¨:   {figures_dir}")
    logger.info("=" * 70)
    print()

    if not result_csv.exists():
        logger.error(f"âŒ æœªæ‰¾åˆ°æ ·æœ¬ç»“æœ: {result_csv}")
        logger.error("   è¯·å…ˆè¿è¡Œ --stage evaluate")
        sys.exit(1)

    # åˆ†æ¡¶ç»Ÿè®¡
    stats_df = aggregate_buckets(
        sample_csv=str(result_csv),
        output_csv=str(bucket_csv),
        n_buckets=5,
    )

    if stats_df.empty:
        logger.error("âŒ åˆ†æ¡¶ç»Ÿè®¡ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
        return

    # ç”»å›¾
    print()
    plot_bucket_trends(
        bucket_csv=str(bucket_csv),
        output_dir=str(figures_dir),
    )

    print()
    logger.info("âœ… é˜¶æ®µ 3 å®Œæˆï¼")
    logger.info(f"   ğŸ“„ æ¡¶ç»Ÿè®¡: {bucket_csv}")
    logger.info(f"   ğŸ“ˆ å›¾è¡¨:   {figures_dir}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CLI å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_args():
    parser = argparse.ArgumentParser(
        description="SimpleQA Real-world è¯„æµ‹é“¾è·¯",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--stage",
        choices=["generate", "evaluate", "analyze", "all"],
        default="all",
        help="è¿è¡Œé˜¶æ®µ:\n"
             "  generate  - æ¨¡å‹å¤šæ¬¡é‡‡æ ·\n"
             "  evaluate  - åˆ¤é”™ + å®ä½“ + Jaccard\n"
             "  analyze   - åˆ†æ¡¶ç»Ÿè®¡ + ç”»å›¾\n"
             "  all       - å…¨éƒ¨æµç¨‹",
    )
    parser.add_argument("--first_n", type=int, default=200, help="å–å‰ N æ¡æ ·æœ¬ (é»˜è®¤ 200)")
    parser.add_argument("--n_samples", type=int, default=10, help="æ¯é¢˜é‡‡æ ·æ¬¡æ•° (é»˜è®¤ 10)")
    parser.add_argument("--temperature", type=float, default=0.7, help="é‡‡æ ·æ¸©åº¦ (é»˜è®¤ 0.7)")
    parser.add_argument("--model", type=str, default=None, help="æœ¬åœ°æ¨¡å‹åç§° (é»˜è®¤ Qwen/Qwen2.5-0.5B)")
    parser.add_argument("--run_id", type=str, default=None, help="è¿è¡Œ IDï¼›ä¸ä¼ åˆ™è‡ªåŠ¨æŒ‰æ—¶é—´ç”Ÿæˆ/é€‰æ‹©æœ€æ–°")
    parser.add_argument("--output_dir", type=str, default=None, help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•")
    parser.add_argument("--log_level", type=str, default="INFO", help="æ—¥å¿—çº§åˆ« (é»˜è®¤ INFO)")
    return parser.parse_args()


class PipelineConfig:
    """å°† argparse + Config ç»Ÿä¸€æˆè¿è¡Œé…ç½®"""

    def __init__(self, args):
        from simpleqa_eval.config import Config, OUTPUTS_DIR

        base_cfg = Config()

        self.model_name = args.model or base_cfg.model_name
        self.device = base_cfg.resolve_device()
        self.first_n = args.first_n
        self.n_samples = args.n_samples
        self.temperature = args.temperature
        self.max_new_tokens = base_cfg.max_new_tokens

        self.deepseek_api_key = base_cfg.deepseek_api_key
        self.deepseek_base_url = base_cfg.deepseek_base_url
        self.deepseek_model = base_cfg.deepseek_model

        self.wiki_cache_db = base_cfg.wiki_cache_db
        self.wiki_search_limit = base_cfg.wiki_search_limit

        self.api_retry_times = base_cfg.api_retry_times
        self.api_retry_delay = base_cfg.api_retry_delay
        self.api_call_delay = base_cfg.api_call_delay

        if args.output_dir:
            self.output_root = Path(args.output_dir)
        else:
            self.output_root = OUTPUTS_DIR

        base_cfg.ensure_dirs()
        self.output_root.mkdir(parents=True, exist_ok=True)
        self.runs_root = self.output_root / "runs"
        self.runs_root.mkdir(parents=True, exist_ok=True)
        self.latest_run_path = self.output_root / "latest_run.txt"

        self.run_id = self._resolve_run_id(args.stage, args.run_id)
        self.outputs_dir = self.runs_root / self.run_id
        self.outputs_dir.mkdir(parents=True, exist_ok=True)
        (self.outputs_dir / "figures").mkdir(parents=True, exist_ok=True)

        self.run_record_path = self.outputs_dir / "run_info.json"
        self.created_at = datetime.now().isoformat(timespec="seconds")
        if self.run_record_path.exists():
            try:
                with open(self.run_record_path, "r", encoding="utf-8") as f:
                    old_record = json.load(f)
                self.created_at = old_record.get("created_at", self.created_at)
            except Exception:
                pass

        if args.stage in ("generate", "all"):
            self.latest_run_path.write_text(self.run_id + "\n", encoding="utf-8")

    def _resolve_run_id(self, stage: str, requested_run_id: str | None) -> str:
        if requested_run_id:
            return requested_run_id

        if stage in ("generate", "all"):
            return datetime.now().strftime("%Y%m%d_%H%M%S")

        if self.latest_run_path.exists():
            latest = self.latest_run_path.read_text(encoding="utf-8").strip()
            if latest and (self.runs_root / latest).exists():
                return latest

        run_dirs = sorted(p.name for p in self.runs_root.iterdir() if p.is_dir())
        if run_dirs:
            return run_dirs[-1]

        raise ValueError("æœªæ‰¾åˆ°å¯ç”¨ runã€‚è¯·å…ˆè¿è¡Œ generate/allï¼Œæˆ–æ˜¾å¼ä¼ å…¥ --run_idã€‚")


def _write_run_record(cfg: "PipelineConfig", args, status: str):
    record = {
        "run_id": cfg.run_id,
        "status": status,
        "stage": args.stage,
        "created_at": cfg.created_at,
        "updated_at": datetime.now().isoformat(timespec="seconds"),
        "output_dir": str(cfg.outputs_dir),
        "config": {
            "model_name": cfg.model_name,
            "device": cfg.device,
            "first_n": cfg.first_n,
            "n_samples": cfg.n_samples,
            "temperature": cfg.temperature,
            "max_new_tokens": cfg.max_new_tokens,
            "deepseek_model": cfg.deepseek_model,
        },
    }
    with open(cfg.run_record_path, "w", encoding="utf-8") as f:
        json.dump(record, f, ensure_ascii=False, indent=2)


def main():
    args = parse_args()
    setup_logging(args.log_level)

    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  SimpleQA Real-world è¯„æµ‹é“¾è·¯                                â•‘")
    print("â•‘  å¤ç°: 'When Bias Pretends to Be Truth' ç¬¬4èŠ‚               â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    try:
        cfg = PipelineConfig(args)
    except ValueError as e:
        logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
        sys.exit(1)

    _write_run_record(cfg, args, status="running")

    logger.info("ğŸ”§ è¿è¡Œé…ç½®:")
    logger.info(f"   é˜¶æ®µ:       {args.stage}")
    logger.info(f"   Run ID:     {cfg.run_id}")
    logger.info(f"   æ¨¡å‹:       {cfg.model_name}")
    logger.info(f"   è®¾å¤‡:       {cfg.device}")
    logger.info(f"   æ•°æ®é‡:     å‰ {cfg.first_n} æ¡")
    logger.info(f"   é‡‡æ ·æ¬¡æ•°:   {cfg.n_samples}")
    logger.info(f"   æ¸©åº¦:       {cfg.temperature}")
    logger.info(f"   DeepSeek:   {cfg.deepseek_model}")
    logger.info(f"   è¾“å‡ºç›®å½•:   {cfg.outputs_dir}")
    logger.info(f"   è®°å½•æ–‡ä»¶:   {cfg.run_record_path}")
    print()

    overall_start = time.time()
    try:
        if args.stage in ("generate", "all"):
            stage_generate(cfg)

        if args.stage in ("evaluate", "all"):
            stage_evaluate(cfg)

        if args.stage in ("analyze", "all"):
            stage_analyze(cfg)

        overall_elapsed = time.time() - overall_start
        _write_run_record(cfg, args, status="completed")
        print()
        logger.info(f"ğŸ å…¨éƒ¨å®Œæˆï¼æ€»è€—æ—¶: {overall_elapsed/60:.1f} åˆ†é’Ÿ")
        print()
    except Exception:
        _write_run_record(cfg, args, status="failed")
        raise


if __name__ == "__main__":
    main()
