"""æœ¬åœ°æ¨¡å‹ç”Ÿæˆå™¨

ä½¿ç”¨ Qwen2.5-0.5Bï¼ˆæˆ–å…¶ä»– HuggingFace æ¨¡å‹ï¼‰è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼Œ
å¹¶é€šè¿‡ logits è®¡ç®— self-confidenceã€‚
"""

import logging
import torch
import numpy as np
from typing import List, Tuple, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer

logger = logging.getLogger(__name__)


class LocalModelGenerator:
    """æœ¬åœ° HuggingFace æ¨¡å‹ç”Ÿæˆå™¨"""

    def __init__(self, model_name: str, device: str = "cpu",
                 max_new_tokens: int = 128):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = max_new_tokens
        self.model = None
        self.tokenizer = None

    def load(self):
        """åŠ è½½æ¨¡å‹å’Œ tokenizer"""
        if self.model is not None:
            return

        logger.info(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_name}")
        logger.info(f"   è®¾å¤‡: {self.device}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="left",
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
        ).to(self.device).eval()

        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def generate_answer(
        self,
        question: str,
        temperature: float = 0.7,
        seed: Optional[int] = None,
    ) -> Tuple[str, float]:
        """å•æ¬¡ç”Ÿæˆç­”æ¡ˆï¼Œè¿”å› (ç­”æ¡ˆæ–‡æœ¬, self-confidence)ã€‚

        Self-confidence = exp(mean(log_probs of generated tokens))
        """
        self.load()

        if seed is not None:
            torch.manual_seed(seed)

        # æ„é€  prompt
        prompt = self._build_prompt(question)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_len = inputs["input_ids"].shape[1]

        # ç”Ÿæˆï¼ˆå¸¦ logits è¿”å›ï¼‰
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                temperature=max(temperature, 1e-4),  # é˜²æ­¢ temperature=0 æŠ¥é”™
                do_sample=temperature > 0,
                top_p=0.95 if temperature > 0 else 1.0,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        # æå–ç”Ÿæˆçš„ token ids
        generated_ids = outputs.sequences[0, input_len:]
        answer_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

        # è®¡ç®— self-confidenceï¼šå¯¹æ¯ä¸ªç”Ÿæˆ token çš„ log-prob å–å¹³å‡å† exp
        confidence = self._compute_confidence(outputs.scores, generated_ids)

        return answer_text, confidence

    def sample_answers(
        self,
        question: str,
        n: int = 10,
        temperature: float = 0.7,
    ) -> List[Tuple[str, float]]:
        """æ‰¹é‡é‡‡æ ·ï¼Œä¼˜å…ˆç”¨ num_return_sequences ä¸€æ¬¡ç”Ÿæˆ n æ¡ç»“æœã€‚

        Returns:
            [(ç­”æ¡ˆæ–‡æœ¬, confidence), ...]
        """
        self.load()
        if n <= 0:
            return []

        do_sample = temperature > 0

        torch.manual_seed(42)

        prompt = self._build_prompt(question)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        input_len = inputs["input_ids"].shape[1]

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                temperature=max(temperature, 1e-4) if do_sample else 1.0,
                do_sample=do_sample,
                top_p=0.95 if do_sample else 1.0,
                num_return_sequences=n if do_sample else 1,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        # outputs.sequences: (n, seq_len)
        # outputs.scores: tuple of (n, vocab_size) tensors, one per generated position
        results = []
        if not do_sample:
            generated_ids = outputs.sequences[0, input_len:]
            answer_text = self.tokenizer.decode(
                generated_ids, skip_special_tokens=True
            ).strip()
            confidence = self._compute_confidence(
                outputs.scores, generated_ids, seq_idx=0
            )
            return [(answer_text, confidence) for _ in range(n)]

        for seq_idx in range(n):
            generated_ids = outputs.sequences[seq_idx, input_len:]
            answer_text = self.tokenizer.decode(
                generated_ids, skip_special_tokens=True
            ).strip()
            confidence = self._compute_confidence(
                outputs.scores, generated_ids, seq_idx
            )
            results.append((answer_text, confidence))

        return results

    def _build_prompt(self, question: str) -> str:
        """æ„é€ é—®ç­” prompt"""
        # ä½¿ç”¨ chat templateï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰ï¼Œå¦åˆ™ç”¨ç®€å• prompt
        if hasattr(self.tokenizer, "apply_chat_template"):
            messages = [
                {"role": "system", "content": "You are a helpful assistant. Answer the question concisely and directly. Give only the answer, no explanation."},
                {"role": "user", "content": question},
            ]
            try:
                return self.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            except Exception:
                pass

        return f"Question: {question}\nAnswer:"

    def _compute_confidence(
        self,
        scores: tuple,
        generated_ids: torch.Tensor,
        seq_idx: int = 0,
    ) -> float:
        """
        ä»ç”Ÿæˆ token çš„ logits è®¡ç®— self-confidenceã€‚

        confidence = exp( mean( log_softmax(logits)[token_id] ) )
        å³ç”Ÿæˆåºåˆ—çš„å‡ ä½•å¹³å‡æ¦‚ç‡ã€‚

        Args:
            scores: generate è¿”å›çš„ scoresï¼Œæ¯ä¸ªå…ƒç´  shape ä¸º (batch, vocab_size)
            generated_ids: å½“å‰åºåˆ—çš„ç”Ÿæˆ token ids
            seq_idx: å½“å‰åºåˆ—åœ¨ batch ä¸­çš„ç´¢å¼•
        """
        if len(scores) == 0:
            return 0.0

        log_probs = []
        n_tokens = min(len(scores), len(generated_ids))

        for i in range(n_tokens):
            logits = scores[i][seq_idx]  # shape: (vocab_size,)
            log_softmax = torch.log_softmax(logits, dim=-1)
            token_id = generated_ids[i].item()

            # è·³è¿‡ç‰¹æ®Š token
            if token_id == self.tokenizer.eos_token_id:
                break
            if token_id == self.tokenizer.pad_token_id:
                continue

            log_prob = log_softmax[token_id].item()
            log_probs.append(log_prob)

        if len(log_probs) == 0:
            return 0.0

        mean_log_prob = np.mean(log_probs)
        confidence = float(np.exp(mean_log_prob))

        return confidence
